{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence embedding - från start till bert\n",
    "<img src='images/bert-ernie-sesame-street.jpg' style='width:800px'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kort om utvecklingen inom NLP\n",
    "* Word2vec – embeddings\n",
    "    * fastText och GloVe\n",
    "    * doc2vec\n",
    "\n",
    "        \n",
    "* seq2seq neurala nätverk\n",
    "    * Encoder-decoder nätverk\n",
    "    * Attention\n",
    "    * Transformer model\n",
    "    * BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec \n",
    "https://arxiv.org/abs/1301.3781\n",
    "\n",
    "Representera ord utifrån vilka ord som används i samma sammanhang\n",
    "\n",
    "”show me your friends, and I'll tell who you are”\n",
    "\n",
    "<img src='images/word2vec.png' style='width:600px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText och GloVe\n",
    "\n",
    "* Användbara paket för träning av word2vec modeller\n",
    "* Har förtränade modeller på olika språk\n",
    "<img src='images/fasttext.png' style='width:400px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup fastText\n",
    "```bash\n",
    "\n",
    "%%bash \n",
    "pip install -U tensorflow==1.14.0 gensim==3.8.0 fasttext==0.9.1 \n",
    "\n",
    "wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip'\n",
    "mkdir fasttext \n",
    "unzip 'wiki-news-300d-1M.vec.zip' -d fasttext\n",
    "rm 'wiki-news-300d-1M.vec.zip'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_sim(s1, s2):\n",
    "    return np.dot(s1,s2)/(np.linalg.norm(s1)*np.linalg.norm(s2)+1e-9)\n",
    "\n",
    "\n",
    "def get_fasttext_wv(word, model, vec_len=300):\n",
    "    try:\n",
    "        wv = model[word]\n",
    "    except:\n",
    "        wv = np.zeros(vec_len)\n",
    "    return wv\n",
    "\n",
    "\n",
    "def fasttext_doc2vec(string, model, stop_words=list(), weights=dict(), smoothing_factor=1, n_dim=300):\n",
    "    \"\"\"\n",
    "    sentence embedding from a word embedding model and string sentence. \n",
    "    args:\n",
    "    string(str): the string to be embedded\n",
    "    model(gensim.models.keyedvectors.Word2VecKeyedVectors): the model\n",
    "    stop_words(list): optional stop words to be removed\n",
    "    weights(dict): optional dict with weights of words \n",
    "    smoothing_factor(int): helps smoothing the doc2 vec. A nonzero value is as \n",
    "    if all words without an explicit weight would have that value as explict. A smoothing factor=0 \n",
    "    is equivalent to ignoring words that has no explcit weight. Set to 1 if unsure.\n",
    "    \n",
    "    \"\"\"\n",
    "    words = set(re.findall(\"[\\w\\d'-]+\", string.lower())) # ignores multiple uses of a word in the doc\n",
    "    \n",
    "    word_weights = []\n",
    "    if words:\n",
    "        word_vectors = [get_fasttext_wv(word, model) for word in words if word not in stop_words]\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_weights.append(weights[word])\n",
    "            except:\n",
    "                word_weights.append(smoothing_factor)\n",
    "                \n",
    "        se = np.mean([vec / (np.linalg.norm(vec) + 1e-9) * weight for vec, weight in zip(word_vectors, word_weights)], axis = 0)\n",
    "    else:\n",
    "        print('Zero doc2vec vector for: ' + string)\n",
    "        se = np.zeros(n_dim)\n",
    "    return se  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import numpy as np\n",
    "filename = 'fasttext/wiki-news-300d-1M.vec'\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similarity between \n",
      "\n",
      "(1) What is your favourite hobby?\n",
      "(2) Wat do you really like to do in your spare time?\n",
      "is: 0.8130461087004068\n"
     ]
    }
   ],
   "source": [
    "## text att jämföra\n",
    "string_1 = 'What is your favourite hobby?'\n",
    "string_2 = 'Wat do you really like to do in your spare time?'\n",
    "\n",
    "\n",
    "str_1_emb = fasttext_doc2vec(string_1, fasttext_model)\n",
    "str_2_emb = fasttext_doc2vec(string_2, fasttext_model)\n",
    "\n",
    "s1_s2_sim_fasttext = cosine_sim(str_1_emb, str_2_emb)\n",
    "\n",
    "print('Document similarity between \\n\\n(1) {s1}\\n(2) {s2}\\nis: {sim}'.format(s1=string_1,\n",
    "                                                                       s2=string_2,\n",
    "                                                                       sim=s1_s2_sim_fasttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "* Inte helt lätt att skapa representationer av hela meningar/dokument\n",
    "* BoW\n",
    "* Viktade medelvektorer\n",
    "* Doc2VecC - https://arxiv.org/pdf/1707.02377.pdf\n",
    "* WMD - http://proceedings.mlr.press/v37/kusnerb15.pdf\n",
    "* Svårt att hantera ordning på ord och homonymer (banan, rabatt, springa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "* RNN till RNN \n",
    "* Två moduler – Encoder och decoder\n",
    "<img src='images/encdec.gif' style='width:600px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder network\n",
    "<img src='images/enc-dec network.PNG' style='width:1000px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder network  med attention\n",
    "<img src='images/attention layer.PNG' style='width:1000px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is all you need – Transformers \n",
    "> https://arxiv.org/abs/1706.03762 \n",
    "\n",
    "> http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "<img src='images/the_transformer_3.png' style='width:1000px'/>\n",
    "<img src='images/transformer_4.png' style='width:500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer multi-head attention\n",
    "\n",
    "<img src='images/transform20fps.gif' style='width:400px'/>\n",
    "\n",
    "* Inga RNN-moduler --> enklare parallellisering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT \n",
    "> \"Bidirectional Encoder Representations from Transformers\"\n",
    "\n",
    "Fokus på representationerna från encoder-delen ≈ doc2vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%bash \n",
    "pip install bert-serving-server==1.9.6 bert-serving-client==1.9.6\n",
    "wget 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip'\n",
    "mkdir bert \n",
    "unzip 'uncased_L-12_H-768_A-12.zip' -d bert\n",
    "rm 'uncased_L-12_H-768_A-12.zip'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### starta bert-server (i separat terminal)\n",
    "```bash\n",
    "bert-serving-start -model_dir '/home/gurra/coding/bert/bert_tutorial/bert/uncased_L-12_H-768_A-12' -num_worker=1 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc=BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similarity between \n",
      "\n",
      "(1)What is your favourite hobby?\n",
      "(2)Wat do you really like to do in your spare time?\n",
      "is: 0.8376639668328283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s1 = bc.encode([string_1])\n",
    "s2 = bc.encode([string_2])\n",
    "s1_s2_sim_bert = cosine_sim(s1[0],s2[0])\n",
    "\n",
    "print('Document similarity between \\n\\n(1){s1}\\n(2){s2}\\nis: {sim}'.format(s1=string_1,\n",
    "                                                                       s2=string_2,\n",
    "                                                                       sim=s1_s2_sim_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vidare forskning\n",
    "\n",
    "* XLnet - https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
