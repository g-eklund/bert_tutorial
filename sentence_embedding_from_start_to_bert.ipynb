{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence embedding - från start till bert\n",
    "\n",
    "<img src='images/bert-ernie-sesame-street.jpg' style='width:800px'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delar i notebooken\n",
    "\n",
    "### Word2vec – embeddings\n",
    "\n",
    "    * fastText och GloVe\n",
    "    * doc2vec\n",
    "    * kodningsexempel - använda fastText \n",
    "    \n",
    "\n",
    "        \n",
    "### seq2seq neurala nätverk\n",
    "\n",
    "    * Encoder-decoder nätverk\n",
    "    * Attention\n",
    "    * Transformer model\n",
    "    * BERT\n",
    "    * kodningsexempel - använda BERT \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec \n",
    "https://arxiv.org/abs/1301.3781\n",
    "\n",
    "Representera ord utifrån vilka ord som används i samma sammanhang\n",
    "\n",
    "”show me your friends, and I'll tell who you are”\n",
    "\n",
    "<img src='images/word2vec.png' style='width:600px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText och GloVe\n",
    "\n",
    "* Användbara paket för träning av word2vec modeller\n",
    "* Har förtränade modeller på olika språk\n",
    "<img src='images/fasttext.png' style='width:400px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup av fastText\n",
    "```bash\n",
    "\n",
    "%%bash \n",
    "pip install -U tensorflow==1.14.0 gensim==3.8.0 fasttext==0.9.1 keras==2.2.5\n",
    "\n",
    "wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip'\n",
    "mkdir fasttext \n",
    "unzip 'wiki-news-300d-1M.vec.zip' -d fasttext\n",
    "rm 'wiki-news-300d-1M.vec.zip'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Använda fastText för document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "\n",
    "\n",
    "def cosine_sim(s1, s2):\n",
    "    return np.dot(s1,s2)/(np.linalg.norm(s1)*np.linalg.norm(s2)+1e-9)\n",
    "\n",
    "\n",
    "def get_fasttext_wv(word, model, vec_len=300):\n",
    "    try:\n",
    "        wv = model[word]\n",
    "    except:\n",
    "        wv = np.zeros(vec_len)\n",
    "    return wv\n",
    "   \n",
    "\n",
    "\n",
    "def fasttext_doc2vec(string, model, stop_words=list(), weights=dict(), smoothing_factor=1, n_dim=300):\n",
    "    \"\"\"\n",
    "    sentence embedding from a word embedding model and string sentence. \n",
    "    args:\n",
    "    string(str): the string to be embedded\n",
    "    model(gensim.models.keyedvectors.Word2VecKeyedVectors): the model\n",
    "    stop_words(list): optional stop words to be removed\n",
    "    weights(dict): optional dict with weights of words \n",
    "    smoothing_factor(int): helps smoothing the doc2 vec. A nonzero value is as \n",
    "    if all words without an explicit weight would have that value as explict. A smoothing factor=0 \n",
    "    is equivalent to ignoring words that has no explcit weight. Set to 1 if unsure.\n",
    "    \n",
    "    \"\"\"\n",
    "    string = ' '.join(string.split(' ')[0:256])# only take 256 first words (to make it fair with bert)\n",
    "    words = set(re.findall(\"[\\w\\d'-]+\", string.lower())) # ignores multiple uses of a word in the doc\n",
    "    \n",
    "    word_weights = []\n",
    "    if words:\n",
    "        word_vectors = [get_fasttext_wv(word, model) for word in words if word not in stop_words]\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_weights.append(weights[word])\n",
    "            except:\n",
    "                word_weights.append(smoothing_factor)\n",
    "                \n",
    "        se = np.mean([vec / (np.linalg.norm(vec) + 1e-9) * weight for vec, weight in zip(word_vectors, word_weights)], axis = 0)\n",
    "    else:\n",
    "        print('Zero doc2vec vector for: ' + string)\n",
    "        se = np.zeros(n_dim)\n",
    "    return se  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fasttext/wiki-news-300d-1M.vec'\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similarity between \n",
      "\n",
      "(1) The man is robbing the bank\n",
      "(2) The guy is stealing from the financial institution\n",
      "is: 0.893739540349151\n"
     ]
    }
   ],
   "source": [
    "## text att jämföra\n",
    "string_1 = 'The man is robbing the bank'\n",
    "string_2 = 'The guy is stealing from the financial institution'\n",
    "\n",
    "str_1_emb = fasttext_doc2vec(string_1, fasttext_model)\n",
    "str_2_emb = fasttext_doc2vec(string_2, fasttext_model)\n",
    "\n",
    "s1_s2_sim_fasttext = cosine_sim(str_1_emb, str_2_emb)\n",
    "\n",
    "print('Document similarity between \\n\\n(1) {s1}\\n(2) {s2}\\nis: {sim}'.format(s1=string_1,\n",
    "                                                                       s2=string_2,\n",
    "                                                                       sim=s1_s2_sim_fasttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Använda fasttext för analys av IMDB reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "%%bash\n",
    "wget http://mng.bz/0tIo\n",
    "mkdir imdb \n",
    "unzip '0tIo' -d imdb\n",
    "rm '0tIo'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def load_raw_imdb(imdb_dir = '/home/gurra/code/private/bert/imdb', data_type='train'):\n",
    "    train_dir = os.path.join(imdb_dir, data_type)\n",
    "\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(train_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "    return texts, labels\n",
    "\n",
    "def make_simple_model(input_dim):       \n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=input_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(100))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(80))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    y=np.asarray(y)\n",
    "    assert X.shape[0] == y.shape[0], 'data is not the same length'\n",
    "    ind = list(range(X.shape[0]))\n",
    "    shuffle(ind)\n",
    "    return X[ind,:].copy(), y[ind].copy()\n",
    "\n",
    "\n",
    "def compile_and_train(model, X_train, y_train):\n",
    "    optimizer = optimizers.RMSprop(lr=0.0003, rho=0.9, epsilon=None, decay=0.001)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.1, shuffle=True, verbose = 0)\n",
    "    for i in hist.history.keys():\n",
    "        print('%s %.3f' % (i,hist.history[i][-1]))\n",
    "    \n",
    "    print('Best validation accuracy: %.3f ' % max(hist.history['val_acc']))\n",
    "    return model, hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, y_train = load_raw_imdb(data_type='train')\n",
    "X_train = np.asarray([fasttext_doc2vec(sent, fasttext_model) for sent in train_texts])\n",
    "X_fasttext, y_fasttext = shuffle_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.332\n",
      "val_acc 0.860\n",
      "loss 0.297\n",
      "acc 0.879\n",
      "Best validation accuracy: 0.862 \n"
     ]
    }
   ],
   "source": [
    "word2vec_model = make_simple_model(X_train.shape[1])\n",
    "w2v_mod, w2v_hist = compile_and_train(word2vec_model, X_fasttext, y_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summering av doc2vec\n",
    "* Inte helt lätt att skapa representationer av hela meningar/dokument\n",
    "* BoW\n",
    "* Viktade medelvektorer\n",
    "* Doc2VecC - https://arxiv.org/pdf/1707.02377.pdf\n",
    "* WMD - http://proceedings.mlr.press/v37/kusnerb15.pdf\n",
    "* Svårt att hantera ordning på ord och homonymer (banan, rabatt, springa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq \n",
    "\n",
    "* RNN till RNN \n",
    "* Två moduler – Encoder och decoder\n",
    "<img src='images/encdec.gif' style='width:600px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder network\n",
    "<img src='images/enc-dec network.PNG' style='width:1000px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder network  med attention\n",
    "<img src='images/attention layer.PNG' style='width:1000px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is all you need – Transformers \n",
    "> https://arxiv.org/abs/1706.03762 \n",
    "\n",
    "> http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "<img src='images/the_transformer_3.png' style='width:1000px'/>\n",
    "<img src='images/transformer_4.png' style='width:500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer multi-head attention\n",
    "\n",
    "<img src='images/transform20fps.gif' style='width:400px'/>\n",
    "\n",
    "* Inga RNN-moduler --> enklare parallellisering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT \n",
    "\n",
    "> \"Bidirectional Encoder Representations from Transformers\"\n",
    "\n",
    "Fokus på representationerna från encoder-delen ≈ doc2vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### komma igång"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%bash \n",
    "pip install bert-serving-server==1.9.6 bert-serving-client==1.9.6\n",
    "wget 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip'\n",
    "mkdir bert \n",
    "unzip 'uncased_L-12_H-768_A-12.zip' -d bert\n",
    "rm 'uncased_L-12_H-768_A-12.zip'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### starta bert-server (i separat terminal)\n",
    "```bash\n",
    "bert-serving-start -model_dir '/home/gurra/coding/bert/bert_tutorial/bert/uncased_L-12_H-768_A-12' -num_worker 8 -batch_size 10 -max_seq_len 256 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Använda BERT för sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc=BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similarity between \n",
      "\n",
      "(1)The man is robbing the bank\n",
      "(2)The guy is stealing from the financial institution\n",
      "is: 0.9162930302304907\n"
     ]
    }
   ],
   "source": [
    "string_1 = 'The man is robbing the bank'\n",
    "string_2 = 'The guy is stealing from the financial institution'\n",
    "\n",
    "\n",
    "s1 = bc.encode([string_1])\n",
    "s2 = bc.encode([string_2])\n",
    "s1_s2_sim_bert = cosine_sim(s1[0],s2[0])\n",
    "\n",
    "print('Document similarity between \\n\\n(1){s1}\\n(2){s2}\\nis: {sim}'.format(s1=string_1,\n",
    "                                                                       s2=string_2,\n",
    "                                                                       sim=s1_s2_sim_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Använda BERT för IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train_bert = bc.encode(train_texts) \n",
    "X_train_bert = pickle.load(open('X_bert.pkl', 'rb'))\n",
    "X_bert,y_bert=shuffle_data(X_train_bert, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.277\n",
      "val_acc 0.892\n",
      "loss 0.217\n",
      "acc 0.918\n",
      "Best validation accuracy: 0.894 \n"
     ]
    }
   ],
   "source": [
    "bert_model = make_simple_model(X_train_bert.shape[1])\n",
    "bert_mod, _ = compile_and_train(bert_model, X_bert, y_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText comparison:\n",
      "val_loss 0.332\n",
      "val_acc 0.860\n",
      "loss 0.297\n",
      "acc 0.879\n",
      "Best validation accuracy: 0.862 \n"
     ]
    }
   ],
   "source": [
    "print('fastText comparison:')\n",
    "for i in w2v_hist.history.keys():\n",
    "    print('%s %.3f' % (i,w2v_hist.history[i][-1]))\n",
    "\n",
    "print('Best validation accuracy: %.3f ' % max(w2v_hist.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vidare forskning\n",
    "\n",
    "* XLnet - https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
